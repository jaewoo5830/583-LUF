{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Tools\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46307bdf",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbad03",
   "metadata": {},
   "source": [
    "## Combine Original Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['label0.csv', 'label1.csv', 'label2_1.csv', 'label2_2.csv', 'label3.csv', 'label3_split0.csv', 'label3_split1.csv', 'label3_split2.csv', 'label3_split3.csv']\n",
    "label_rank_df = pd.DataFrame()\n",
    "label_rank_df = pd.concat([pd.read_csv(f, header = None, names=['filename', 1, 2, 4, 8, 16 , 32, 64, 128]) for f in files], ignore_index = True)\n",
    "label_rank_df.set_index('filename', inplace = True)\n",
    "label_rank_df.to_csv(\"combined_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the average.py to get the average_combined_labels.cv\n",
    "averaged_label_df = pd.read_csv(\"averaged_combined_labels.csv\", header=None, names=['filename', 1, 2, 4, 8, 16 , 32, 64, 128])\n",
    "averaged_label_df.set_index(\"filename\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecab92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "averaged_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ba8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rank(row):\n",
    "    row_list = np.array(row)\n",
    "    index_sorted = np.argsort(row_list)\n",
    "    ranking = [2**i for i in index_sorted]\n",
    "    for i, rank in enumerate(ranking):\n",
    "        row[rank] = i+1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_rank_df = averaged_label_df.apply(lambda x : find_rank(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_rank_df.to_csv(\"label_rank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c90864",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd90078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv(\"full_features.csv\") # change to corresponding feature csv\n",
    "feature_df = feature_df.set_index(\"Filename\")\n",
    "label_df = pd.read_csv(\"label_final.csv\") # change to corresponding label csv\n",
    "label_df = label_df.set_index(\"Filename\")\n",
    "dataset_df = feature_df.join(label_df, on=\"Filename\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077203ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Depth', 'TripCount', 'Total', 'FP', 'BR', 'Mem', 'Uses', 'Defs']\n",
    "X = dataset_df.loc[:, feature_cols].to_numpy()\n",
    "y = dataset_df[\"Optimal Unroll Factor\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [DecisionTreeClassifier(), RandomForestClassifier(n_estimators=500), SVC(), LinearSVC(dual='auto'), KNeighborsClassifier(), MLPClassifier(), XGBClassifier(n_estimators=500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X[:int(X.shape[0]*0.9)], X[int(X.shape[0]*0.1):]\n",
    "# y_train, y_test = y[:int(X.shape[0]*0.9)], y[int(X.shape[0]*0.1):]\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_predicted = clf.predict(X_test)\n",
    "# metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(clf):\n",
    "    scores = []\n",
    "    rank_pred = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if type(clf).__name__ == \"XGBClassifier\":\n",
    "            y_train = le.fit_transform(y_train)\n",
    "            y_test = le.fit_transform(y_test)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predicted = clf.predict(X_test)\n",
    "        scores.append(metrics.accuracy_score(y_test, y_predicted))\n",
    "        \n",
    "        # store the ranks\n",
    "        filenames = pd.Series(dataset_df.index.to_list())[test_index]\n",
    "        if type(clf).__name__ == \"XGBClassifier\":\n",
    "            ranks = [label_rank_df.loc[f][2**y] for f, y in zip(filenames, y_predicted)]\n",
    "        else:\n",
    "            ranks = [label_rank_df.loc[f][y] for f, y in zip(filenames, y_predicted)]\n",
    "        rank_pred.append(ranks)\n",
    "\n",
    "    # Rank analysis\n",
    "    rank_counts = dict(collections.Counter(list(itertools.chain.from_iterable(rank_pred))))\n",
    "    total_sum = sum(rank_counts.values())\n",
    "    averaged_dict = {key: value / total_sum for key, value in rank_counts.items()}\n",
    "\n",
    "    result = {\n",
    "        'Model': type(clf).__name__,\n",
    "        \"Accuracy %\": np.array(scores).mean() * 100,\n",
    "        \"Top 3 Prediction %\": (averaged_dict[1] + averaged_dict[2] + averaged_dict[3]) * 100\n",
    "    }\n",
    "    # top2_percentage = (averaged_dict[1] + averaged_dict[2]) * 100\n",
    "    # accuracy_average = np.array(scores).mean() * 100\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = pd.DataFrame(columns=[\"Model\", \"Accuracy %\", \"Top 3 Prediction %\"])\n",
    "for clf in clfs:\n",
    "    model_result = pd.concat([model_result, pd.DataFrame([model_performance(clf)])],ignore_index=True)\n",
    "model_result.set_index(\"Model\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ead51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result.to_csv(\"model_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "rank_pred = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_predicted))\n",
    "    \n",
    "    filenames = pd.Series(dataset_df.index.to_list())[test_index]\n",
    "    ranks = [label_rank_df.loc[f][y] for f, y in zip(filenames, y_predicted)]\n",
    "    rank_pred.append(ranks)\n",
    "    \n",
    "# Put the performance of the model on each fold in the scores array\n",
    "np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9613b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_counts = dict(collections.Counter(list(itertools.chain.from_iterable(rank_pred))))\n",
    "\n",
    "total_sum = sum(rank_counts.values())\n",
    "\n",
    "averaged_dict = {key: value / total_sum for key, value in rank_counts.items()}\n",
    "\n",
    "averaged_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
